{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to perform some time series analysis on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import yfinance as yf\n",
    "import os\n",
    "import numpy as np\n",
    "eps_df = pd.read_csv('extracted_data.csv',parse_dates=['start','end','filed'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below downloads data for the range of dates covered by EPS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 1, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data for  AAPL\n"
     ]
    }
   ],
   "source": [
    "for ticker in eps_df['ticker'].unique():\n",
    "    ticker = 'AAPL'\n",
    "    start = eps_df[eps_df['ticker'] == ticker]['start'].min()\n",
    "    end = eps_df[eps_df['ticker'] == ticker]['end'].max()\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(f'./stock_daily_data/{ticker}.csv'):\n",
    "            data = yf.download(ticker, start - timedelta(days=3),\n",
    "                           end + timedelta(days=3), progress=False)\n",
    "            #data.to_csv(f'./stock_daily_data/{ticker}.csv')\n",
    "            print(\"Downloaded data for \", ticker)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to download {ticker}\")\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "\n",
    "    # print(\"Sleeping for 5 seconds\")\n",
    "    # time.sleep(60*60/2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Close', 'High', 'Low', 'Open', 'Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code pads them all to a unified single file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [eps_df['ticker'].unique(), ['Close','High','Low','Open','Volume','Diluted EPS trailing','PE Ratio','filing_type','filing_dEPS','filing_period_start','filing_period_end']]\n",
    "full_df = pd.DataFrame(index=pd.date_range(eps_df['start'].min(), datetime.today(), freq='d'), columns=pd.MultiIndex.from_product(iterables, names=['ticker', 'metric']))\n",
    "full_df.index.name = 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eps_df contains eps values for a given time period for each ticker\n",
    "#this code runs through each line and generates a unified time series data set for each ticker\n",
    "for i, row in eps_df.iterrows():\n",
    "    date_range = pd.date_range(start=row['start'], end=row['end'])\n",
    "    # limit to quarterly earnings only\n",
    "    if len(date_range) >85 and len(date_range) < 95:\n",
    "        #df.loc[date_range, row['ticker']] = row['EarningsPerShareDiluted']\n",
    "        full_df.loc[date_range, (row['ticker'],'Diluted EPS trailing')] = row['EarningsPerShareDiluted']*365/(row['end']-row['start']).days\n",
    "        full_df.loc[row['filed'], (row['ticker'],'filing_type')] = row['form']\n",
    "        full_df.loc[row['filed'], (row['ticker'],'filing_dEPS')] = row['EarningsPerShareDiluted']\n",
    "        full_df.loc[row['filed'], (row['ticker'],'filing_period_start')] = row['start']\n",
    "        full_df.loc[row['filed'], (row['ticker'],'filing_period_end')] = row['end']\n",
    "#df.to_csv('eps_by_date.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Timestamp('2005-12-29 00:00:00'), Timestamp('2005-12-30 00:00:00')] not in index\"\n",
      "Failed to load AES\n",
      "\"[Timestamp('2005-12-29 00:00:00'), Timestamp('2005-12-30 00:00:00')] not in index\"\n",
      "Failed to load MDLZ\n"
     ]
    }
   ],
   "source": [
    "#fill values from the downloaded stock data\n",
    "for ticker in eps_df['ticker'].unique():\n",
    "    try:\n",
    "        data = pd.read_csv(f'./stock_daily_data/{ticker}.csv', index_col='Date', parse_dates=True,header=2)\n",
    "        data.columns = ['Close','High','Low','Open','Volume']\n",
    "        full_df.loc[data.index, (ticker,'Close')] = data['Close']\n",
    "        full_df.loc[data.index, (ticker,'High')] = data['High']\n",
    "        full_df.loc[data.index, (ticker,'Low')] = data['Low']\n",
    "        full_df.loc[data.index, (ticker,'Open')] = data['Open']\n",
    "        full_df.loc[data.index, (ticker,'Volume')] = data['Volume']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to load {ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate PE ratio\n",
    "for ticker in eps_df['ticker'].unique():\n",
    "    for i, row in full_df.iterrows():\n",
    "        if not row[(ticker,'Diluted EPS')]>0  or np.isnan(row[(ticker,'Close')]):\n",
    "            continue\n",
    "        else:\n",
    "            full_df.loc[i,(ticker,'PE Ratio')] = full_df.loc[i,(ticker,'Close')]/full_df.loc[i,(ticker,'Diluted EPS')]#calculate PE ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming full_df is your DataFrame and eps_df contains the tickers\n",
    "\n",
    "for ticker in eps_df['ticker'].unique():\n",
    "    # Filter rows where 'Diluted EPS' > 0 and 'Close' is not NaN\n",
    "    valid_rows = full_df[(full_df[(ticker, 'Diluted EPS trailing')] > 0) & (~full_df[(ticker, 'Close')].isna())]\n",
    "    \n",
    "    # Calculate 'PE Ratio' for the filtered rows\n",
    "    full_df.loc[valid_rows.index, (ticker, 'PE Ratio')] = valid_rows[(ticker, 'Close')] / valid_rows[(ticker, 'Diluted EPS trailing')]\n",
    "\n",
    "# Now full_df has the 'PE Ratio' calculated only for the valid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#plot PE ratio with drop down for ticker\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "#plot PE ratio with drop down for ticker\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "for ticker in eps_df['ticker'].unique():\n",
    "    fig.add_trace(go.Scatter(x=full_df.index, y=full_df[(ticker, 'PE Ratio')], name=ticker), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=full_df.index, y=full_df[(ticker, 'Close')], name=ticker), row=2, col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data quality check \n",
    "#Checking for missing values. sucessful\n",
    "# Assuming new_df is your DataFrame\n",
    "missing_values = {}\n",
    "new_df = df.copy()\n",
    "for column in new_df.columns:\n",
    "    # Drop leading and trailing NaNs\n",
    "    trimmed_col = new_df[column].dropna()\n",
    "    first_valid_index = trimmed_col.first_valid_index()\n",
    "    last_valid_index = trimmed_col.last_valid_index()\n",
    "    \n",
    "    # Slice the column to exclude leading and trailing NaNs\n",
    "    if first_valid_index is not None and last_valid_index is not None:\n",
    "        trimmed_col = trimmed_col.loc[first_valid_index:last_valid_index + timedelta(days=1)]\n",
    "    \n",
    "    # Count the number of missing values in the trimmed column\n",
    "    missing_count = trimmed_col.isna().sum()\n",
    "    missing_values[column] = missing_count\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "for ticker, count in missing_values.items():\n",
    "    if count > 0:\n",
    "        print(f\"{ticker}: {count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length mismatch: Expected axis has 6 elements, new values have 5 elements\n",
      "Failed to load BRK.B.csv\n",
      "Length mismatch: Expected axis has 6 elements, new values have 5 elements\n",
      "Failed to load BF.B.csv\n"
     ]
    }
   ],
   "source": [
    "#One time fix for all CSV files\n",
    "for file in os.listdir('./daily-data/'):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "    try:\n",
    "        data = pd.read_csv(f'./daily-data/{file}', index_col='Date', parse_dates=True,header=2)\n",
    "        data.columns = ['Close','High','Low','Open','Volume']\n",
    "        data.to_csv(f'./daily-data/{file}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Failed to load {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPX-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
